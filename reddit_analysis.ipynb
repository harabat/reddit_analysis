{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising api for pushshift and Reddit\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ['CLIENT_ID'],\n",
    "    client_secret=os.environ['CLIENT_SECRET'],\n",
    "    user_agent=os.environ['USER_AGENT'],\n",
    ")\n",
    "\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining query parameters\n",
    "\n",
    "start_epoch = int(datetime(2020, 1, 1).timestamp())\n",
    "end_epoch = int(datetime(2021, 1, 1).timestamp())\n",
    "subreddit = os.environ['SUBREDDIT']\n",
    "\n",
    "# creating the query\n",
    "\n",
    "submissions_gen = api.search_submissions(\n",
    "    subreddit=subreddit,\n",
    "    after=start_epoch,\n",
    "    before=end_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting the data and pickling it\n",
    "\n",
    "submissions = list(submissions_gen)\n",
    "\n",
    "with open('reddit_submissions', 'wb') as file:\n",
    "    pickle.dump(submissions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpickling the data\n",
    "\n",
    "with open('reddit_submissions', 'rb') as file:\n",
    "    submissions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating dataframe, cleaning it up, and pickling it\n",
    "\n",
    "data_sub = pd.DataFrame(\n",
    "    item.__dict__ for item in list(submissions)\n",
    ")\n",
    "\n",
    "data_sub = data_sub.reindex(columns=sorted(data_sub.columns))\n",
    "\n",
    "data_sub.to_pickle('reddit_data_sub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = pd.read_pickle('reddit_data_sub.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data_sub.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exploring submission data\n",
    "\n",
    "data_sub['date'] = data_sub['created_utc'].apply(\n",
    "    lambda x: datetime.utcfromtimestamp(x))\n",
    "\n",
    "print(data_sub['date'])\n",
    "print(sorted(data_sub['score'].unique(), reverse=True))\n",
    "print(sorted(data_sub['num_comments'].unique(), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=data_sub, x='date', y='score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need for PRAW here, only PSAW\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining query parameters\n",
    "start_epoch = int(datetime(2020, 1, 1).timestamp())\n",
    "end_epoch = int(datetime(2021, 1, 1).timestamp())\n",
    "subreddit = os.environ['SUBREDDIT']\n",
    "\n",
    "# creating the query\n",
    "comments_gen = api.search_comments(\n",
    "    subreddit=subreddit,\n",
    "    after=start_epoch,\n",
    "    before=end_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting the data and pickling it\n",
    "comments = []\n",
    "\n",
    "for comment in comments_gen:\n",
    "    comments.append(comment)\n",
    "    if not len(comments) % 1000:\n",
    "        print(len(comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating dataframe, cleaning it up, and pickling it\n",
    "data_com = pd.DataFrame(\n",
    "    item.d_ for item in comments\n",
    ")\n",
    "\n",
    "# add date and len columns\n",
    "data_com_full['date'] = data_com_full['created_utc'].apply(\n",
    "    lambda x: datetime.utcfromtimestamp(x))\n",
    "data_com_full['len'] = data_com_full['body'].apply(len)\n",
    "\n",
    "# sort columns alphabetically\n",
    "data_com = data_com.reindex(columns=sorted(data_com.columns))\n",
    "\n",
    "# pickle data\n",
    "data_com.to_pickle('reddit_data_com.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging fragmented data and full data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fragmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve data from pickle\n",
    "\n",
    "data_com_part = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir('PSAW/'):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.startswith('reddit_data_com_0'):\n",
    "        print(filename)\n",
    "        data_com_part = pd.read_pickle('PSAW/' + filename)\n",
    "        data_com_part = data_com_part.append(data_com_part, ignore_index=True)\n",
    "\n",
    "data_com_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date and len columns\n",
    "data_com_part['date'] = data_com_part['created_utc'].apply(\n",
    "    lambda x: datetime.utcfromtimestamp(x))\n",
    "data_com_part['len'] = data_com_part['body'].apply(len)\n",
    "\n",
    "# counting duplicates\n",
    "duplicates = data_com_part.duplicated(\n",
    "    subset=['created_utc', 'author', 'id']\n",
    ")\n",
    "data_com_part.drop_duplicates(\n",
    "    subset=['created_utc', 'author', 'id'],\n",
    "    inplace=True\n",
    ")\n",
    "print(sum(duplicates))\n",
    "\n",
    "# cleaning the data\n",
    "data_com_part = data_com_part.reindex(columns=sorted(data_com_part.columns))\n",
    "data_com_part.sort_values('date', ascending=False, inplace=True)\n",
    "\n",
    "# pickling data\n",
    "data_com_part.to_pickle('reddit_data_com_part.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring comments data\n",
    "\n",
    "print(data_com_part.shape)\n",
    "print(data_com_part['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com_full = pd.read_pickle('reddit_data_com_full.pkl')\n",
    "data_com_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting duplicates\n",
    "duplicates = data_com_full.duplicated(\n",
    "    subset=['created_utc', 'author', 'id']\n",
    ")\n",
    "data_com_full.drop_duplicates(\n",
    "    subset=['created_utc', 'author', 'id'],\n",
    "    inplace=True\n",
    ")\n",
    "print(sum(duplicates))\n",
    "\n",
    "data_com_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring comments data\n",
    "print(data_com_full['date'])\n",
    "print(sorted(data_com_full['score'].unique(), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing fragmented data and full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com_part = pd.read_pickle('reddit_data_com_part.pkl')\n",
    "data_com_full = pd.read_pickle('reddit_data_com_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'data_com:\\t',\n",
    "    data_com_part.shape,\n",
    "    '\\ndata_com_full:\\t',\n",
    "    data_com_full.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com = data_com_part.append(data_com_full, ignore_index=True)\n",
    "\n",
    "data_com['date'] = data_com['created_utc'].apply(\n",
    "    lambda x: datetime.utcfromtimestamp(x))\n",
    "data_com['len'] = data_com['body'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting duplicates\n",
    "duplicates = data_com.duplicated(\n",
    "    subset=['created_utc', 'author', 'id'],\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(sum(duplicates))\n",
    "\n",
    "data_com.drop_duplicates(\n",
    "    subset=['created_utc', 'author', 'id'],\n",
    "    keep='first',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "print(data_com.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com.to_pickle('reddit_data_com.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unpickle data\n",
    "data_com = pd.read_pickle('reddit_data_com.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "data_com.fillna(value=np.nan, inplace=True)\n",
    "data_com.mask(data_com.applymap(str).eq('[]'), other=np.nan, inplace=True)\n",
    "data_com.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "data_com.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "data_com.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep columns relevant columns\n",
    "columns = [\n",
    "    'author',  # author of comment\n",
    "    'body',  # body of comment\n",
    "    #     'created_utc', # date created (UTC) as epoch\n",
    "    'date',  # date created (UTC)\n",
    "    'distinguished',  # moderator comments\n",
    "    'gildings',  # number of times comment was given gold\n",
    "    'id',\n",
    "    'len',  # len of body\n",
    "    'link_id',  # submission id\n",
    "    'parent_id',  # id of comment or submission being replied to\n",
    "    'permalink',  # url of comment\n",
    "    'score',  # score\n",
    "]\n",
    "\n",
    "data_com = data_com.reindex(columns=columns, copy=False)\n",
    "data_com.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize id\n",
    "data_com['link_id'] = data_com['link_id'].apply(lambda x: str(x)[3:])\n",
    "data_com['parent_id'] = data_com['parent_id'].apply(lambda x: str(x)[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most upvoted comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com.sort_values('score', ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in data_com.sort_values('score', ascending=False)[:5].index:\n",
    "    for j in data_com.columns:\n",
    "        print(data_com.iloc[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments that generated the most discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_id is the relevant submission\n",
    "# parent_id is the parent item (comment or submission)\n",
    "first_level = data_com['link_id'] == data_com['parent_id']\n",
    "\n",
    "print(\n",
    "    'First level comments:\\t', sum(first_level),\n",
    "    '\\nReply comments:\\t', sum(~first_level)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiliaze dict from first-level comment ids\n",
    "most_active = dict.fromkeys(\n",
    "    data_com[first_level]['id'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# loop over parent_id until every comment has been assigned to original first-level one\n",
    "mask = [True] * len(data_com)\n",
    "n = -1\n",
    "\n",
    "while n != sum(most_active.values()):\n",
    "    n = sum(most_active.values())\n",
    "\n",
    "    ids = data_com[mask]['parent_id']\n",
    "    mask = data_com['id'].isin(ids)\n",
    "\n",
    "    for i in data_com[mask]['id']:\n",
    "        try:\n",
    "            most_active[i] += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "# turn dict into dataframe\n",
    "most_active = pd.DataFrame.from_dict(\n",
    "    most_active,\n",
    "    orient='index',\n",
    "    columns=['Replies']\n",
    ").sort_values('Replies', ascending=False)\n",
    "\n",
    "most_active.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data_com[first_level]['id']) == set(most_active.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com[data_com['id'].isin(most_active[:5].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How common is it to get replies to a comment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_prop = pd.DataFrame(\n",
    "    {\n",
    "        'reply_count': range(most_active.values.min(), most_active.values.max() + 1),\n",
    "        'proportion_less': [\n",
    "            sum(most_active['Replies'] == i) / len(most_active) for i in range(\n",
    "                most_active.values.min(), most_active.values.max() + 1\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ").set_index('reply_count')\n",
    "\n",
    "replies_prop.plot.bar()\n",
    "sns.barplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_active\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "sns.barplot(data=replies_prop, ax=ax)\n",
    "# ax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(most_active, log_scale=[False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderators = data_com['distinguished'] == 'moderator'\n",
    "\n",
    "not_moderators = data_com['distinguished'] != 'moderator'\n",
    "sum(not_moderators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = sorted(data_com['author'].unique())\n",
    "members[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com.groupby('author').count()['distinguished'].sort_values()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_active_members = data_com.groupby('author').count()['score']\n",
    "most_active_members.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_upvoted_members = data_com.groupby('author').sum()['score']\n",
    "most_upvoted_members.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_members = pd.DataFrame({\n",
    "    'members': members,\n",
    "    'most_active': most_active_members,\n",
    "    'most_upvoted': most_upvoted_members\n",
    "})\n",
    "\n",
    "data_members.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion of active members against subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_members = 360000\n",
    "\n",
    "proportion = len(data_members) / total_members\n",
    "\n",
    "print(\n",
    "    'Proportion of active members vs total of subscribers:\\t{:.1%}'.format(proportion)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Most active members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_members[data_members['most_active'] > 1000]['most_active'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most upvoted members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    data=data_members,\n",
    "    x='members',\n",
    "    y='most_upvoted'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and pickling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ['CLIENT_ID'],\n",
    "    client_secret=os.environ['CLIENT_SECRET'],\n",
    "    user_agent=os.environ['USER_AGENT'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions = list(reddit.subreddit(os.environ['SUBREDDIT']).top(limit=1000))\n",
    "\n",
    "# comments = [i.comments for i in submissions]\n",
    "\n",
    "# with open('PRAW/reddit_submissions', 'wb') as file:\n",
    "#     pickle.dump(submissions, file)\n",
    "\n",
    "# with open('PRAW/reddit_comments', 'wb') as file:\n",
    "#     pickle.dump(comments, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('PRAW/reddit_submissions', 'rb') as file:\n",
    "#     submissions = pickle.load(file)\n",
    "\n",
    "# with open('PRAW/reddit_comments', 'rb') as file:\n",
    "#     comments = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sub = pd.DataFrame(\n",
    "#     item.__dict__ for item in submissions\n",
    "# )\n",
    "\n",
    "# data_sub = data_sub.reindex(columns=sorted(data_sub.columns))\n",
    "\n",
    "# data_sub.to_pickle('PRAW/reddit_data_sub')\n",
    "\n",
    "# data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_com = pd.DataFrame(\n",
    "#     comment.__dict__ for item in comments for comment in item.list()\n",
    "# )\n",
    "\n",
    "# data_com = data_com.reindex(columns=sorted(data_com.columns))\n",
    "\n",
    "# data_com.to_pickle('PRAW/reddit_data_com')\n",
    "\n",
    "# data_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking validity of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = pd.read_pickle('PRAW/reddit_data_sub')\n",
    "data_com = pd.read_pickle('PRAW/reddit_data_com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = ['Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "data_sub['Day'] = data_sub['created_utc'].apply(\n",
    "    lambda x:\n",
    "    days[\n",
    "        datetime.utcfromtimestamp(x).weekday()\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_sub['Hour'] = data_sub['created_utc'].apply(\n",
    "    lambda x:\n",
    "        datetime.utcfromtimestamp(x).hour\n",
    ")\n",
    "\n",
    "data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com = data_com[data_com['created_utc'] > 0]\n",
    "\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thur', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "data_com['Day'] = data_com['created_utc'].apply(\n",
    "    lambda x:\n",
    "    days[\n",
    "        datetime.utcfromtimestamp(x).weekday()\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_com['Hour'] = data_com['created_utc'].apply(\n",
    "    lambda x:\n",
    "        datetime.utcfromtimestamp(x).hour\n",
    ")\n",
    "\n",
    "data_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(x):\n",
    "    if len(x) > 10:\n",
    "        return np.median(x)\n",
    "\n",
    "\n",
    "def mean(x):\n",
    "    if len(x) > 10:\n",
    "        return np.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Highest posting activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_activity = data_sub.pivot_table(\n",
    "    index='Day',\n",
    "    columns='Hour',\n",
    "    values='score',\n",
    "    aggfunc=len\n",
    ").reindex(index=days)\n",
    "\n",
    "posting_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    posting_activity,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    square=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Number of submissions\\ndepending on day of week and hour of posting',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most commenting activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commenting_activity = data_com.pivot_table(\n",
    "    index='Day',\n",
    "    columns='Hour',\n",
    "    values='created_utc',\n",
    "    aggfunc=len\n",
    ").reindex(index=days)\n",
    "\n",
    "commenting_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    commenting_activity,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    square=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Number of comments\\ndepending on day of week and hour',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commenting to posting ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = commenting_activity / posting_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    ratio,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    square=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Commenting to posting ratio\\ndepending on day of week and hour',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximising comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.groupby('Day')['num_comments'].mean().reindex(index=days).plot()\n",
    "data_sub.groupby('Day')['num_comments'].median().reindex(\n",
    "    index=days).plot(color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.groupby('Hour')['num_comments'].mean().plot()\n",
    "data_sub.groupby('Hour')['num_comments'].median().plot(color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By day of week and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_median_comments = data_sub.pivot_table(\n",
    "    index='Day',\n",
    "    columns='Hour',\n",
    "    values='num_comments',\n",
    "    aggfunc=mean\n",
    ").reindex(index=days)\n",
    "\n",
    "timing_median_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    timing_median_comments,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    square=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Median comments per submission\\ndepending on day of week and hour of posting',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximising score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.groupby('Day')['score'].mean().reindex(index=days).plot()\n",
    "data_sub.groupby('Day')['score'].median().reindex(index=days).plot(color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### By hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.groupby('Hour')['score'].mean().plot()\n",
    "data_sub.groupby('Hour')['score'].median().plot(color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By day of week and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_median_score = data_sub.pivot_table(\n",
    "    index='Day',\n",
    "    columns='Hour',\n",
    "    values='score',\n",
    "    aggfunc=mean\n",
    ").reindex(index=days)\n",
    "\n",
    "timing_median_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    timing_median_score,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    square=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Median score per submission\\ndepending on day of week and hour of posting',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data_sub[(data_sub['Day'] == 'Thur') & (\n",
    "    data_sub['num_comments'] >= 10)]['Hour'], bins=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.drop(\n",
    "    columns=[\n",
    "        'allow_live_comments',\n",
    "        'all_awardings',  # related to awards received\n",
    "        'approved_by',  # indicates comments approved or banned by moderators\n",
    "        'archived',\n",
    "        'approved_at_utc',  # indicates comments approved or banned by moderators\n",
    "        'author_cakeday',  # account creation anniversary\n",
    "        'author_flair_background_color',  # related to flair\n",
    "        'author_flair_css_class',  # related to flair\n",
    "        'author_flair_richtext',  # related to flair\n",
    "        'author_flair_template_id',  # related to flair\n",
    "        'author_flair_text',  # related to flair\n",
    "        'author_flair_text_color',  # related to flair\n",
    "        'author_flair_type',  # related to flair\n",
    "        'author_fullname',  # unique id for author\n",
    "        'author_patreon_flair',  # related to flair\n",
    "        #         'author_premium', # whether account is premium\n",
    "        'awarders',  # related to awards received, mostly empty\n",
    "        'banned_at_utc',  # indicates comments approved or banned by moderators\n",
    "        'banned_by',  # indicates comments approved or banned by moderators\n",
    "        'can_gild',\n",
    "        'can_mod_post',\n",
    "        'category',\n",
    "        'clicked',\n",
    "        'comment_limit',\n",
    "        'comment_sort',\n",
    "        'content_categories',\n",
    "        'contest_mode',\n",
    "        'created',  # equivalent to created_utc\n",
    "        #         'created_utc', # date created (UTC)\n",
    "        'crosspost_parent',\n",
    "        'crosspost_parent_list',\n",
    "        'discussion_type',\n",
    "        #         'distinguished', # identifies moderators\n",
    "        'edited',  # indicates edited comments\n",
    "        'flair',\n",
    "        'gilded',\n",
    "        'gildings',  # number of times comment was given gold\n",
    "        'hidden',\n",
    "        'hide_score',\n",
    "        'id',  # unique id for comment\n",
    "        'is_crosspostable',\n",
    "        'is_meta',  # empty\n",
    "        'is_original_content',  # empty\n",
    "        'is_robot_indexable',\n",
    "        'is_submitter',  # whether commented is also submitter\n",
    "        'is_video',  # empty\n",
    "        'likes',  # empty\n",
    "        'link_flair_background_color',  # no added information\n",
    "        'link_flair_css_class',  # no added information\n",
    "        'link_flair_richtext',  # no added information\n",
    "        'link_flair_template_id',  # no added information\n",
    "        'link_flair_text_color',  # no added information\n",
    "        'link_flair_type',  # no added information\n",
    "        #         'link_id', # unique id for submission\n",
    "        'locked',  # identifies deleted messages\n",
    "        'media_embed',  # same as media\n",
    "        'media_metadata',  # same as media\n",
    "        'media_only',  # empty\n",
    "        'mod',  # empty\n",
    "        'mod_note',  # empty\n",
    "        'mod_reason_by',  # empty\n",
    "        'mod_reason_title',  # empty\n",
    "        'mod_reports',  # empty\n",
    "        'no_follow',\n",
    "        #         'num_crossposts', # no information\n",
    "        #         'num_duplicates', # no information\n",
    "        'num_reports',  # empty\n",
    "        'over_18',  # empty\n",
    "        #         'parent_id', # unique id of comment or submission being replied to\n",
    "        'parent_whitelist_status',  # same value\n",
    "        'permalink',  # url of comment\n",
    "        'pinned',  # no added information\n",
    "        'poll_data',  # few values\n",
    "        'post_hint',  # few values\n",
    "        'pwls',  # same value\n",
    "        'quarantine',  # no added information\n",
    "        'removal_reason',  # empty\n",
    "        'removed_by',  # empty\n",
    "        'removed_by_category',  # empty\n",
    "        'report_reasons',  # empty\n",
    "        'retrieved_on',  # date that the comment was scraped by pushshift\n",
    "        'saved',  # empty\n",
    "        #         'score', # score\n",
    "        'secure_media',  # same as media\n",
    "        'secure_media_embed',  # same as media\n",
    "        'send_replies',  # mainly indicates archived threads, removed comments, AutoModerator comments\n",
    "        'selftext_html',  # same as selftext\n",
    "        'subreddit',  # related to subreddit\n",
    "        'subreddit_id',  # unique id for subreddit\n",
    "        'subreddit_name_prefixed',  # related to subreddit\n",
    "        'subreddit_subscribers',  # related to subreddit\n",
    "        'subreddit_type',  # related to subreddit\n",
    "        'send_replies',  # few values\n",
    "        'spoiler',  # not relevant\n",
    "        'steward_reports',  # no information\n",
    "        'stickied',  # moderator comments\n",
    "        'suggested_sort',  # few values\n",
    "        'thumbnail',  # depends on domain\n",
    "        'thumbnail_height',  # no added information\n",
    "        'thumbnail_width',  # no added information\n",
    "        'total_awards_received',  # total awards received\n",
    "        'top_awarded_type',  # empty\n",
    "        'treatment_tags',  # no information, empty\n",
    "        'url_overridden_by_dest',  # same as 'domain' not self,  'self' == False\n",
    "        'user_reports',  # empty\n",
    "        'view_count',  # empty\n",
    "        'visited',  # empty\n",
    "        'whitelist_status',  # same value\n",
    "        'wls',  # same value\n",
    "        '_comments',\n",
    "        '_comments_by_id',\n",
    "        '_fetched',\n",
    "        '_reddit'\n",
    "    ],\n",
    "    errors='ignore',\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "data_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories that should have correlations\n",
    "\n",
    "# self-post\n",
    "data_sub[data_sub['selftext'].apply(len) > 0]['domain'].value_counts()\n",
    "# title-only self-post\n",
    "data_sub[data_sub['is_self'] == True]['domain'].value_counts()\n",
    "\n",
    "# reddit media\n",
    "data_sub[data_sub['is_reddit_media_domain'] == True]['domain'].value_counts()\n",
    "\n",
    "# non-reddit media\n",
    "data_sub[data_sub['media'].apply(bool) == True]['domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dummy variables here?\n",
    "\n",
    "data_sub.replace(\n",
    "    to_replace={\n",
    "        False: 0,\n",
    "        True: 1,\n",
    "        np.NaN: 0,\n",
    "        None: 0,\n",
    "        '': 0\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "data_sub.replace(\n",
    "    to_replace='.+',\n",
    "    value=1,\n",
    "    regex=True,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "data_sub.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairwise correlation table\n",
    "\n",
    "corr_data_sub = data_sub.corr().dropna(how='all').dropna(axis=1, how='all')\n",
    "\n",
    "corr_data_sub.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# mask for strong correlations\n",
    "\n",
    "mask = corr_data_sub.applymap(lambda x: abs(x) > 0.45 and x < 1)\n",
    "\n",
    "mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of strong correlations\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_data_sub[mask].dropna(how='all').dropna(axis=1, how='all'),\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    linewidths=.5,\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    #     square=True,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    labels=ax.get_xticklabels(),\n",
    "    ha='right',\n",
    "    rotation=30\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Most correlated values',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for weak correlations\n",
    "\n",
    "mask_inverse = corr_data_sub.applymap(lambda x: abs(x) < 0.45)\n",
    "\n",
    "mask_inverse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of weak correlations\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    figsize=(15, 10)\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    corr_data_sub[mask_inverse].dropna(how='all').dropna(axis=1, how='all'),\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    linewidths=.5,\n",
    "    cmap='YlOrBr',\n",
    "    cbar=False,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(\n",
    "    labels=ax.get_xticklabels(),\n",
    "    ha='right',\n",
    "    rotation=30\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    'Least correlated values',\n",
    "    fontsize='x-large',\n",
    "    weight='semibold'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissions = list(reddit.subreddit(os.environ['SUBREDDIT']).top(limit=1000))\n",
    "\n",
    "# comments = [i.comments for i in submissions]\n",
    "\n",
    "# with open('PRAW/reddit_submissions_top', 'wb') as file:\n",
    "#     pickle.dump(submissions, file)\n",
    "\n",
    "# with open('PRAW/reddit_comments_top', 'wb') as file:\n",
    "#     pickle.dump(comments, file)\n",
    "\n",
    "with open('PRAW/reddit_submissions_top', 'rb') as file:\n",
    "    submissions_top = pickle.load(file)\n",
    "\n",
    "with open('PRAW/reddit_comments_top', 'rb') as file:\n",
    "    comments_top = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_top = pd.DataFrame(\n",
    "    i.__dict__ for i in submissions_top\n",
    ")\n",
    "\n",
    "data_sub_top = data_sub_top.reindex(columns=sorted(data_sub_top.columns))\n",
    "\n",
    "data_sub_top.to_pickle('PRAW/reddit_data_sub_top')\n",
    "\n",
    "data_sub_top.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_com_top = pd.DataFrame(\n",
    "    comment.__dict__ for item in comments_top for comment in item.list()\n",
    ")\n",
    "\n",
    "data_com_top = data_com_top.reindex(columns=sorted(data_com_top.columns))\n",
    "\n",
    "data_com_top.to_pickle('PRAW/reddit_data_com_top')\n",
    "\n",
    "data_com_top.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = pd.read_csv(\n",
    "    './languages.csv')\n",
    "\n",
    "languages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_full = ' '.join(list(str(i).lower() for i in data_com_top['body']))\n",
    "\n",
    "comments_tokenize = findall('[a-z]+', comments_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "\n",
    "for language in languages['Language']:\n",
    "    freq_language = comments_tokenize.count(language.lower())\n",
    "    freq[language] = freq_language\n",
    "    if language.find('/') > -1:\n",
    "        freq[language] += comments_tokenize.count(\n",
    "            language.split('/')[1].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_freq = pd.DataFrame(\n",
    "    {\n",
    "        'Language': freq.keys(),\n",
    "        'Count': freq.values()\n",
    "    }\n",
    ").sort_values('Count', ascending=False, ignore_index=True)\n",
    "\n",
    "languages_freq['Percentage'] = languages_freq['Count'] / \\\n",
    "    languages_freq['Count'].sum() * 100\n",
    "\n",
    "languages_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Count', y='Language',\n",
    "            data=languages_freq[languages_freq['Percentage'] > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_full = ' '.join(list(str(i).lower() for i in (\n",
    "    data_sub_top['title'] + ' ' + data_sub_top['selftext'])))\n",
    "\n",
    "submissions_tokenize = findall('[a-z]+', submissions_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = {}\n",
    "\n",
    "for language in languages['Language']:\n",
    "    freq_language = submissions_tokenize.count(language.lower())\n",
    "    freq[language] = freq_language\n",
    "    if language.find('/') > -1:\n",
    "        freq[language] += submissions_tokenize.count(\n",
    "            language.split('/')[1].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_freq = pd.DataFrame(\n",
    "    {\n",
    "        'Language': freq.keys(),\n",
    "        'Count': freq.values()\n",
    "    }\n",
    ").sort_values('Count', ascending=False, ignore_index=True)\n",
    "\n",
    "languages_freq['Percentage'] = languages_freq['Count'] / \\\n",
    "    languages_freq['Count'].sum() * 100\n",
    "\n",
    "languages_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Count', y='Language',\n",
    "            data=languages_freq[languages_freq['Percentage'] > 1])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "668px",
    "width": "347px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "346.85px",
    "left": "1230px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
